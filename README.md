# WIPS-IPClassification : 특허 자동 분류기(LLM, QLoRA)

LLM과 프롬프트 엔지니어링, QLoRA 파인튜닝을 활용해
사용자가 업로드한 특허 데이터를 자동으로 분석하고,
사용자가 원하는 분류 카테고리에 맞게 분류해주는 특허 자동 분류 시스템입니다.

- 원하는 분류 방식을 선택(프롬프트 엔지니어링, 파인 튜닝)
- 분류를 원하는 특허 문서 정보가 담긴 엑셀 또는 csv 파일 업로드
- 이후 선택한 분류 방식으로 분류 진행
- 분류 결과를 엑셀로 다운로드

Streamlit 기반 UI를 통해 웹 서비스와 같은 GUI로 분류기 서비스를 제공합니다.

## 주요 기능 요약
- 분류 방식 선택(프롬프트 엔지니어링, 파인 튜닝)
- 분류를 원하는 특허 문서들의 내용이 정리된 파일 업로드
- 분류에 사용할 컬럼을 자유롭게 선택 가능
- (프롬프트 엔지니어링) 분류 카테고리를 자유롭게 CRUD 가능
- (파인 튜닝) 분류를 진행할 모델과 그 모델에서 사용할 레이어 수 조정 가능(10% 단위)
- 분류가 완료되면 카테고리 별 분류 결과 및 분류 분포를 UI로 확인 가능
- 분류 결과 엑셀 다운로드 기능 제공

## 프로젝트 사용 예시(예상 동작 흐름)
- 사용자가 분류 방식을 선택한 다음 분류를 원하는 파일을 업로드
- 분류 방식에 따른 분류 진행
  - 프롬프트 엔지니어링
    - 업로드한 파일의 미리보기를 확인하고, 프롬프트에 포함시킬 컬럼 선택
    - 분류할 카테고리의 이름과 설명을 원하는 갯수만큼 작성
    - 기본적으로 제시하는 프롬프트를 사용하거나 수정 가능
    - LLM 분류를 위해 LM Studio 서버가 작동 중 인지 서버의 URL과 LLM 모델 이름을 넣어 연결 체크
    - 분류 진행
    - 분류 결과 확인 및 희망 시 엑셀 다운로드 
  - 파인 튜닝
    - 추론
      - 추론에 사용할 컬럼 선택
      - 추론에 사용할 모델 선택(프로젝트 실행 경로 하위의 models 경로 자동 탐색 또는 모델 경로 직접 입력)
      - chunking 세팅 설정
      - 분류 진행
      - 분류 결과 확인 및 희망 시 엑셀 다운로드
    - 학습
      - 학습에 사용할 모델 이름과 hugging face 토큰 입력
      - 학습에 사용할 컬럼 선택
      - 입력한 모델의 레이어 사용율 선택(10% 단위)
      - 하이퍼 파라미터 설정(양자화, LoRA, SFT 설정 파라미터)
      - chunking 세팅 설정
      - 모델 저장 경로와 저장할 모델 이름 설정
      - 학습 eval 지표 확인

## (프롬프트 엔지니어링) 예시 분류 카테고리
| 카테고리명 | 설명 |
|-----------|-----|
| CPC_C01B | 일반 무기화학 반응과 화합물 관련 특허 코드 |
| CPC_C01C | 금속 화합물과 금속화학 관련 특허 코드 |
| CPC_C01D | 무기 화합물 제조 방법 관련 특허 코드 |
| CPC_C01F | 무기 화합물의 분석, 분리, 정제 관련 특허 코드 |
| CPC_C01G | 비금속 무기 화합물 및 그 응용 관련 특허 코드 |

## 발표 PPT
[여기](https://drive.google.com/file/d/1IUZNFqNpJwyeFsJP2QCtt7kEs29mlTzA/view?usp=sharing)를 클릭하면 DMC 코넷에서 발표를 진행했던 PPT 자료를 확인할 수 있습니다.

## 프로젝트 실행 순서
1. Repository clone
```bash
git clone https://github.com/Junyoung-Choi-0212/WIPS-IPClassification.git
cd WIPS-IPClassification
```
2. 가상환경 구성 및 패키지 설치
```bash
pip install -r requirements.txt
```
3. .env 파일 생성
```bash
# .env 예시
HF_TOKEN=your_key_here
```
4. 실행
```bash
streamlit run app.py
```

프롬프트 엔지니어링으로 분류를 원할 경우 LM Studio에서 Serve on Local Network 설정을 켜주세요.
해당 설정을 켜지 않으면 Streamlit 앱에서 LLM 서버와 연결 및 통신이 되지 않아 분류를 할 수 없습니다.

### Serve on Local Network 설정 활성화 방법
1. LM Studio 실행
2. 좌측의 Developer 탭으로 이동
3. 서버 실행 UI 옆의 Settings 클릭
4. Serve on Local Network 설정 활성화

## 프로젝트 코드 구성
| 폴더명       | 분류                                     |
|-------------|-----------------------------------------|
| components  | 단일 UI 중심의 코드                       |
| data        | 프로젝트 개발 및 테스트에 사용한 특허 데이터  |
| methods     | 특허 분류 방법(프롬프트, 파인튜닝) 구분      |
| tabs        | 파인튜닝 페이지 중 추론, 학습 탭            |
| tests       | 기능 적용 전 테스트 용 코드                |
| utils       | 단일 기능 중심의 코드                      |

## 프로젝트 체크리스트
- [x] Github를 활용한 프로젝트 관리
  → 전체 프로젝트를 GitHub 기반으로 버전 관리하며 `.env`, `.py` 모듈, Streamlit UI, LLM 구성을 모듈화하여 커밋 히스토리 관리.
- [x] 문제 정의에서 시작하는 프로젝트 시작
  → 특허 문서의 급격한 증가 및 특화된 도메인의 고도화로 인해 LLM의 필요성이 높아지고 있다.
- [x] DMC 코넷 과정 또는 멘토링 진행 과정에서 배웠던 기술, 스킬 
  → LLM을 단순 챗봇이 아니라 선택한 분류 방법에 맞게 고도화 하여 사용
    - 프롬프트 엔지니어링
      - 프롬프트를 2단계로 나누어 보다 정확하게 사용자가 원하는 카테고리로 분류하기 위해 질의
    - 파인 튜닝
      - 낮은 PC의 사양에서도 상대적으로 더 좋은 모델을 사용할 수 있게 하기 위해 레이어 사용률 조정
      - GPU 메모리 절약을 위한 양자화 + LoRA 학습
      - 사용자가 제공하는 데이터를 놓치지 않기 위한 Window Sliding Chunking 방식 채용
- [x] 문제 정의 → LLM → 해결
  → 전체 구조를 `문제 정의 → 분류 방식 선택 → 분류 데이터 업로드 → 분류 방식에 따른 세팅 → LLM 분류` 흐름으로 완성.  
    Streamlit 기반 UI로 사용자 입력을 받고, LLM 답변을 분류 상세 확인을 위한 expander UI + 분류 분포 확인을 위한 파이그래프로 구성.

## 프로젝트 학습 포인트
- LLM과 QLoRA를 활용한 특허 분류기 구조를 처음부터 직접 구성
- 다양한 특허 분야에 대응 가능한 분류 카테고리 설정(프롬프트 엔지니어링), 업로드한 데이터로 학습(파인 튜닝) 하는 시스템 설계
- 메모리 절약을 위해 LLM을 양자화 하여 LoRA 학습을 진행할 뿐만 아니라, 레이어 사용률을 지정해 적은 수의 레이어를 사용하는 모델 사용 설계
- chunking을 하면 하나의 특허라 하더라도 각 chunk가 잘린 부분이 어딘지에 따라 분류가 나뉘는 경우가 있는데, 이를 soft voting으로 취합하는 시스템 설계
- Streamlit에서 파이 그래프를 노출하고, 해당 그래프를 엑셀에 이미지로 첨부하는 기능 구현

## 향후 확장 아이디어
- 엑셀 다운로드 시 제공하는 데이터를 보다 면밀히 검토하여 첨삭 진행
- UX & UI 업데이트
- 분류 시간 감축 방법 고안
- 현재 모델의 레이어 경로 후보군을 하드코딩하여 비교하는데, 이를 자동화 할 방안 모색

## 상세 기술 스택
| 기술명                     | 버전       | 설명                        |
|---------------------------|------------|----------------------------|
| bitsandbytes              | v0.48.0.dev0  | LLM 모델 양자화 세팅      |
| LM Studio                 | v0.3.24       | 로컬에 LLM 모델 다운로드  |
| peft                      | v0.17.1       | LoRA 학습 설정           |
| plotly                    | v6.3.0        | 분류 결과 파이그래프 생성  |
| Python                    | v3.12.11      | 전체 백엔드 및 LLM 처리   |
| python-dotenv             | v1.1.1        | API 키 및 환경 변수 관리  |
| scikit-learn              | v1.7.1        | 모델 eval 결과 추출      |
| Streamlit                 | v1.49.1       | 사용자 인터페이스 구성    |
| transformers              | v4.56.0       | LLM 모델 로딩 등         |

## 참고 도서
- Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow(2판)
- Hands-On Large Language Models
- Fluent Python(2판)
